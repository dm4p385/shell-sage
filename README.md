# ğŸ¤– ShellSage

> âœ¨ Context-aware, low-latency command-line autocomplete powered by Trie, FAISS, and LLMs.

ShellSage is my attempt at an intelligent autocomplete engine for your terminal that combines the speed of Trie-based search, the semantic understanding of FAISS, and the reasoning capabilities of LLMs to provide blazing-fast, contextually relevant command suggestions.
> NOTE: This is only supported in fish at the moment
---

## ğŸš€ Features

- âš¡ **Low-latency Suggestions** using Trie prefix search  
- ğŸ§  **Context-aware Search** using FAISS semantic embeddings  
- ğŸ¤– **LLM-Powered Completions** to refine and personalize suggestions  
- ğŸ“š **User Command History Integration**  
- ğŸ“ƒï¸ **Hybrid Ranking Engine** combining all sources  
- ğŸ§µ **gRPC-based Streaming API** for interactive clients  
- ğŸ’¾ **FAISS Cache** to reuse LLM results and reduce API latency  
- ğŸ” **Modular, extensible engine** built in Python

---

## ğŸ—ï¸ Architecture

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Terminal  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
               gRPC backend client
                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   ShellSage Core  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚             â”‚              â”‚              â”‚
TrieSearch   FaissSearch   LLMCompletion   FaissCache
(prefix)     (semantic)     (GPT-like)     (embedding cache)
```

---

## âš™ï¸ Installation

```bash
# Clone the repo
git clone https://github.com/your-username/shellsage.git
cd shellsage

# Set up virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Install ollama
<install ollama>

# Pull mistral
ollama pull mistral
```

---

## ğŸ¥ª Running the Server

```bash
python -m src.api.server
```

This will start the **gRPC server** on `localhost:50051`. It uses a thread pool executor and gracefully handles streamed prompts from clients.

---

## ğŸ§  Core Components

| Component         | Description                                |
|------------------|--------------------------------------------|
| `TrieSearch`      | Fast prefix matching from shell history    |
| `FaissSearch`     | Vector search using sentence embeddings    |
| `LLMCompletion`   | Calls local AI model to refine suggestions |
| `FaissCache`      | Embedding-keyed cache to avoid redundant LLM calls |
| `Ranker`          | Combines and scores all results            |
| `gRPC Server`     | Streams back autocomplete results to clients |

---


## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ api/                 # gRPC server & protobufs
â”œâ”€â”€ prediction_engine/   # Core engine components
â”‚   â”œâ”€â”€ core/            # Trie, FAISS, LLM logic
â”‚   â”œâ”€â”€ data/            # History loader
â”‚   â””â”€â”€ ranking/         # Result ranking strategy
â”œâ”€â”€ utils/               # Logging, helpers
â”œâ”€â”€ protobuf/            # Generated gRPC code
```

---

## ğŸ“ TODO

- [ ] Shell plugin for Bash/Zsh/Fish autocompletion
- [ ] Persistent command history across sessions
- [ ] Create a proper RAG for LLM (might increase latency)
- [ ] Session-aware completions

---


## ğŸ“„ License

MIT License Â© 2025 Harsh S.

